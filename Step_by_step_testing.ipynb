{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # operating system functions like renaming files and directories\n",
    "import subprocess  # for python to interact with the HDFS\n",
    "import shutil  # recursive file and directory operations\n",
    "import glob  # pattern matching for paths\n",
    "import pandas as pd  # data mangling and transforming\n",
    "import bandicoot as bc  # MIT toolkit for creating bandicoot indicators\n",
    "import argparse  # entering flags from the cmd line\n",
    "import gnuper as gn  # gnuper package for creating cdr features\n",
    "from pyspark.sql import SparkSession  # using spark context for big data files\n",
    "from pyspark.sql.functions import col  # needed for function over each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_flag = True\n",
    "bc_flag = False\n",
    "hdfs_flag = True\n",
    "verbose = True\n",
    "clean_up = False\n",
    "raw_data_path = 'CDR/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define attributes for this session\n",
    "att = gn.Attributes(mp_flag=mp_flag,\n",
    "                    bc_flag=bc_flag,\n",
    "                    hdfs_flag=hdfs_flag,\n",
    "                    verbose=verbose,\n",
    "                    clean_up=clean_up,\n",
    "                    raw_data_path=raw_data_path,\n",
    "                    max_chunksize=500,\n",
    "                    cap_coords=[15.500654, 32.559899],  # capital gps\n",
    "                    weekend_days=[5, 6],\n",
    "                    sparkmaster='yarn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # --- Part 1 --- (Preprocessing of raw files and saving by user)\n",
    "# spark = SparkSession.builder.master(att.sparkmaster)\\\n",
    "#     .appName('cdr_extraction_part1').getOrCreate()\n",
    "# print('Spark environment for Part 1 created!')\n",
    "\n",
    "# # ## antennas datasets\n",
    "# # read cell and antenna locations into a spark dataframe (sdf)\n",
    "# raw_locations = gn.read_as_sdf(file=att.raw_locations,\n",
    "#                                sparksession=spark, header=False,\n",
    "#                                 colnames=['cell_id', 'antenna_id',\n",
    "#                                           'longitude', 'latitude'],\n",
    "#                                 query=gn.queries.general.raw_locations_query())\n",
    "# # create raw table to query and cache it as we will query it for every day\n",
    "# raw_locations.createOrReplaceTempView('table_raw_locations')\n",
    "# spark.catalog.cacheTable('table_raw_locations')\n",
    "\n",
    "# # solely antenna locations as sdf (= ignore cell_id)\n",
    "# # FILE: save as 1 csv next to the raw data as we will need it later on\n",
    "# raw_locations.selectExpr('antenna_id', 'longitude', 'latitude')\\\n",
    "#     .dropDuplicates().write.csv(att.antennas_file,\n",
    "#                                 mode='overwrite', header=True)\n",
    "# print('Antenna SDF & table created!')\n",
    "\n",
    "# # ## Preprocessing\n",
    "# # **Level 0**: General preprocessing of raw call detail records\n",
    "# # Storing daily files in a unified dataframe\n",
    "# print('Starting with Level 0: General preprocessing of raw CDRs.')\n",
    "\n",
    "# # ### CDR datasets\n",
    "# if att.verbose:\n",
    "#     dates = gn.files_in_folder(folder=att.raw_data_path, file_pattern='20*.csv',\n",
    "#                                hdfs_flag=att.hdfs_flag)\n",
    "#     dates = [os.path.basename(d).replace('.csv', '') for d in dates]\n",
    "#     print('Available CDR Dates: '+str(dates))  # doublechecking\n",
    "\n",
    "# # order of the raw columns\n",
    "# raw_colnames = ['CALL_RECORD_TYPE', 'CALLER_MSISDN', 'CALL_DATE',\n",
    "#                 'BASIC_SERVICE', 'MS_LOCATION', 'CALL_PARTNER_IDENTITY_TYPE',\n",
    "#                 'CALL_PARTNER_IDENTITY', 'TAC_CODE', 'CALL_DURATION']\n",
    "\n",
    "# # reading in user_ids\n",
    "# users = gn.sdf_from_folder(folder=att.raw_data_path, attributes=att,\n",
    "#                            sparksession=spark, file_pattern='20*.csv',\n",
    "#                            header=False, colnames=raw_colnames,\n",
    "#                            query=gn.queries.general.get_user_ids_query(), action='union')\n",
    "\n",
    "# # drop duplicate ids and create table to query\n",
    "# users = users.dropDuplicates()\n",
    "# users.createOrReplaceTempView('table_user_ids')\n",
    "# # create chunk id for every user, based on max users per chunk\n",
    "# users_w_chunk = spark.sql(gn.queries.general.chunking_query(\n",
    "#                               table_name='table_user_ids',\n",
    "#                               max_chunksize=att.max_chunksize))\n",
    "# users_w_chunk.createOrReplaceTempView('table_chunk_ids')\n",
    "# # cache this table as well due to querying it for every day\n",
    "# spark.catalog.cacheTable('table_chunk_ids')\n",
    "\n",
    "# # preprocess every single file and save it split by chunk id\n",
    "# gn.sdf_from_folder(folder=att.raw_data_path, attributes=att,\n",
    "#                    sparksession=spark, file_pattern='20*.csv',\n",
    "#                    header=False,\n",
    "#                    colnames=raw_colnames,\n",
    "#                    query=gn.queries.level0.raw_preprocessing_query(\n",
    "#                            cump=att.call_unit_multiplicator),\n",
    "#                    save_path=att.chunking_path, save_format='csv',\n",
    "#                    save_header=True, save_mode='overwrite',\n",
    "#                    save_partition='chunk_id', action='save')\n",
    "# print('CSV files created for user chunks.')\n",
    "\n",
    "# # clear cached tables immediately\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "# if not att.hdfs_flag:\n",
    "#     # proper file naming and directory structure\n",
    "#     # get all subdirectories in the chunking folder\n",
    "#     subdirs = next(os.walk(att.chunking_path))[1]\n",
    "\n",
    "#     for d in subdirs:\n",
    "#         c = d.replace('chunk_id=', '')  # extract chunk id\n",
    "#         filenames = glob.glob(att.chunking_path+d+'/*.csv')  # get all files\n",
    "#         combined_csv = pd.concat([pd.read_csv(f) for f in filenames])  # unite them\n",
    "#         # save as new file with chunk id\n",
    "#         combined_csv.to_csv(att.chunking_path+c+'.csv', index=False)\n",
    "#         shutil.rmtree(att.chunking_path+d)  # delete obsolete dir\n",
    "#     print('CSV files united and renamed for user chunks.')\n",
    "    \n",
    "# spark.stop()\n",
    "# print('DONE with Part 1! User chunks saved to chunking folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # --- Part 2 --- (create antenna indicators in a loop over the chunks)\n",
    "# spark = SparkSession.builder.master(att.sparkmaster)\\\n",
    "#     .appName('cdr_extraction_part2').getOrCreate()\n",
    "# print('Spark environment for Part 2 created!')\n",
    "\n",
    "# # ## antenna locations\n",
    "# # read in table which was created in part 1\n",
    "# antennas_locations = spark.read.csv(att.antennas_file,\n",
    "#                                     header=True,\n",
    "#                                     inferSchema=True)\n",
    "\n",
    "# # create raw table to query and cache it for several queries\n",
    "# antennas_locations.createOrReplaceTempView('table_antennas_locations')\n",
    "# spark.catalog.cacheTable('table_antennas_locations')\n",
    "\n",
    "# # ## User chunks\n",
    "# if att.hdfs_flag:\n",
    "#     args = \"hdfs dfs -ls -R \"+att.chunking_path+\" | grep drwx | awk -F'/' '{print $NF}'\"\n",
    "#     p = subprocess.Popen(args,\n",
    "#                          shell=True,\n",
    "#                          stdout=subprocess.PIPE,\n",
    "#                          stderr=subprocess.STDOUT)\n",
    "#     s_output, s_err = p.communicate()\n",
    "#     chunks = s_output.decode('utf-8').split()\n",
    "# else:\n",
    "#     chunks = sorted(glob.glob(attributes.chunking_path))\n",
    "# # save length for loop\n",
    "# n = len(chunks)\n",
    "# print('Available user chunks: '+str(n))\n",
    "\n",
    "# # collect home antennas per chunk\n",
    "# home_antennas_list = []\n",
    "\n",
    "# # ## LOOP\n",
    "# # ### --- LOOP START ---\n",
    "# for chunk in chunks:\n",
    "#     i = chunks.index(chunk)+1\n",
    "#     print('Starting with '+chunk+', iteration '+str(i)+' out of '+str(n))\n",
    "\n",
    "#     # read in chunk and filter out machines and multi-users\n",
    "#     raw_df = gn.sdf_from_folder(att.chunking_path+chunk+'/', att, spark, \n",
    "#                                 header=True, inferSchema=False,\n",
    "#                                 query=gn.queries.level0.filter_machines_query(\n",
    "#                                 max_weekly_interactions=att.\n",
    "#                                 max_weekly_interactions))\n",
    "\n",
    "#     # quick look\n",
    "#     if att.verbose and i == 1:\n",
    "#         # double check file format\n",
    "#         print('Raw aggregated SDF structure:')\n",
    "#         raw_df.show(10)\n",
    "\n",
    "#     # and register as table\n",
    "#     raw_df.createOrReplaceTempView('table_raw_df')\n",
    "\n",
    "#     # ## home antennas\n",
    "#     user_home_antenna_df = spark.sql(gn.queries.level1.user_home_antenna_query(\n",
    "#                                      noct_time=att.noct_time,\n",
    "#                                      table_name='table_raw_df'))\n",
    "#     # save as table\n",
    "#     user_home_antenna_df.createOrReplaceTempView('table_user_home_antenna_df')\n",
    "#     # and append to ongoing list\n",
    "#     home_antennas_list.append(user_home_antenna_df)\n",
    "\n",
    "#     # unique users in this chunk (doublechecking)\n",
    "#     n_users = raw_df.select('caller_id').distinct().count()\n",
    "#     n_users_w_ha = user_home_antenna_df.select('user_id').distinct().count()\n",
    "#     print('Ratio of users in this chunk with home antenna: '+str(n_users_w_ha/n_users))\n",
    "\n",
    "#     # repartitioning by user\n",
    "#     raw_df = raw_df.repartition(n_users, 'caller_id')\n",
    "\n",
    "#     # **Level 1**: Intermediate tables on user level (user_id still visible)\n",
    "\n",
    "#     # Datasets created in first iteration with columns in brackets:\n",
    "#     # + **user_metrics**: metrics aggregated per user_id, day and hour\n",
    "#     #  *(user_id, day, hour, og_calls, ic_calls, og_sms, ic_sms,\n",
    "#     #    og_vol, ic_vol)*\n",
    "#     # + **user_home_antenna**: monthly estimate of antenna closest to home\n",
    "#     #  location per user *(user_id, antenna_id, month)*\n",
    "#     # + **user_bandicoot_features**: bandicoot interactions on user level per\n",
    "#     #  month *(user_id, month, ...)*\n",
    "\n",
    "#     print('Starting with Level 1: Intermediate tables on user level \\\n",
    "#     (user_id still visible).')\n",
    "\n",
    "#     # #### user_metrics\n",
    "#     user_metrics_df = spark.sql(gn.queries.level1.user_metrics_query(\n",
    "#                                 table_name='table_raw_df'))\n",
    "#     user_metrics_df.createOrReplaceTempView('table_user_metrics_df')\n",
    "\n",
    "#     # #### bandicoot_metrics\n",
    "#     if att.bc_flag and not att.hdfs_flag:\n",
    "#         # remove files from potential previous run\n",
    "#         if os.path.exists(attributes.bandicoot_path):\n",
    "#             shutil.rmtree(attributes.bandicoot_path)\n",
    "\n",
    "#         bc_metrics_df = spark.sql(gn.queries.level1.bc_metrics_query(\n",
    "#                                   table_name='table_raw_df'))\n",
    "\n",
    "#         # define unique users\n",
    "#         users = [str(u.caller_id) for u in bc_metrics_df.select('caller_id')\n",
    "#                  .dropDuplicates().collect()]\n",
    "\n",
    "#         # save single user files\n",
    "#         bc_metrics_df.coalesce(1).write.save(\n",
    "#                     path=att.bandicoot_path,\n",
    "#                     format='csv',\n",
    "#                     header=True,\n",
    "#                     mode='overwrite',\n",
    "#                     partitionBy='caller_id')\n",
    "\n",
    "#         # proper file naming and directory structure\n",
    "#         # get all subdirectories in bandicoot folder\n",
    "#         subdirs = next(os.walk(attributes.bandicoot_path))[1]\n",
    "\n",
    "#         for d in subdirs:\n",
    "#             u = d.replace('caller_id=', '')  # extract user id\n",
    "#             # rename csv to user id and move up\n",
    "#             os.rename(glob.glob(attributes.bandicoot_path+d+'/*.csv')[0],\n",
    "#                       attributes.bandicoot_path+u+'.csv')\n",
    "#             shutil.rmtree(attributes.bandicoot_path+d)  # delete obsolete dir\n",
    "#         print('Single User Files for bandicoot created for chunk '+str(i))\n",
    "\n",
    "#         # execute bandicoot calculation as batch\n",
    "#         indicators = gn.bc_batch(users, attributes)\n",
    "\n",
    "#         # save as csv\n",
    "#         bc.io.to_csv(indicators, '../bandicoot_indicators_'+str(i)+'.csv')\n",
    "#         print('Bandicoot files csvs created for chunk '+str(i))\n",
    "\n",
    "#         # re-read as single sdf\n",
    "#         bc_metrics_df = spark.read.csv('../bandicoot_indicators_' + str(i) +\n",
    "#                                        '.csv', header=True, inferSchema=True)\n",
    "\n",
    "#         # cleaning\n",
    "#         bc_metrics_df = bc_metrics_df\\\n",
    "#             .drop('reporting__antennas_path',\n",
    "#                   'reporting__attributes_path',\n",
    "#                   'reporting__recharges_path',\n",
    "#                   'reporting__version',\n",
    "#                   'reporting__code_signature',\n",
    "#                   'reporting__groupby',\n",
    "#                   'reporting__split_week',\n",
    "#                   'reporting__split_day',\n",
    "#                   'reporting__start_time',\n",
    "#                   'reporting__end_time',\n",
    "#                   'reporting__night_start',\n",
    "#                   'reporting__night_end',\n",
    "#                   'reporting__weekend',\n",
    "#                   'reporting__number_of_antennas',\n",
    "#                   'reporting__bins',\n",
    "#                   'reporting__bins_with_data',\n",
    "#                   'reporting__bins_without_data',\n",
    "#                   'reporting__has_call',\n",
    "#                   'reporting__has_text',\n",
    "#                   'reporting__has_home',\n",
    "#                   'reporting__has_recharges',\n",
    "#                   'reporting__has_attributes',\n",
    "#                   'reporting__has_network',\n",
    "#                   'reporting__number_of_recharges',\n",
    "#                   'reporting__percent_records_missing_location',\n",
    "#                   'reporting__antennas_missing_locations',\n",
    "#                   'reporting__percent_outofnetwork_calls',\n",
    "#                   'reporting__percent_outofnetwork_texts',\n",
    "#                   'reporting__percent_outofnetwork_contacts',\n",
    "#                   'reporting__percent_outofnetwork_call_durations',\n",
    "#                   'reporting__ignored_records__all',\n",
    "#                   'reporting__ignored_records__interaction',\n",
    "#                   'reporting__ignored_records__direction',\n",
    "#                   'reporting__ignored_records__correspondent_id',\n",
    "#                   'reporting__ignored_records__datetime',\n",
    "#                   'reporting__ignored_records__call_duration',\n",
    "#                   'reporting__ignored_records__location')\n",
    "\n",
    "#         # clean up to save space\n",
    "#         if attributes.clean_up:\n",
    "#             shutil.rmtree(attributes.bandicoot_path)\n",
    "\n",
    "#     # **Level 2**: Intermediate tables on antenna level\n",
    "#     # (user_id NOT visible anymore)\n",
    "\n",
    "#     # Datasets created in second iteration with columns in brackets:\n",
    "#     # + **antenna_interactions_generic**: alltime interactions between antennas\n",
    "#     #  without allocation of home antenna locations but generic activity\n",
    "#     #  *(og_antenna_id, ic_antenna_id, sms_count, calls_count, vol_sum)*\n",
    "#     # + **antenna_metrics_week**: metrics aggregated per home antenna of\n",
    "#     #  individual users, week and part of the week\n",
    "#     #  *(antenna_id, week_part, week_number, og_calls, ic_calls, og_sms,\n",
    "#     #  ic_sms, og_vol, ic_vol)*\n",
    "#     # + **antenna_metrics_hourly**: metrics aggregated per home antenna of\n",
    "#     #  individual users and hour\n",
    "#     #  *(antenna_id, hour, og_calls, ic_calls, og_sms, ic_sms, og_vol, ic_vol)*\n",
    "#     # + **antenna_interactions**: alltime interactions between antennas based\n",
    "#     #  on the users' behavior to which a certain antenna is the homebase\n",
    "#     #  *(antenna_id1, antenna_id2, sms_count, calls_count, vol_sum)*\n",
    "#     # + **antenna_bandicoot**: alltime averaged bandicoot interactions on\n",
    "#     #  antenna level *(antenna_id, ...)*\n",
    "#     #\n",
    "#     # A further explanation of the single features can be found\n",
    "#     #  here[http://bandicoot.mit.edu/docs/reference/bandicoot.individual.html]\n",
    "\n",
    "#     print('Starting with Level 2: Intermediate tables on antenna level\\\n",
    "#      (user_id NOT visible anymore).')\n",
    "\n",
    "#     # #### antenna_metrics_week\n",
    "#     antenna_metrics_week_df = spark.sql(gn.queries.level2.antenna_metrics_week_query(\n",
    "#                                         weekend_days=tuple(att.weekend_days),\n",
    "#                                         table_name='table_user_metrics_df'))\n",
    "\n",
    "#     # #### antenna_metrics_hourly\n",
    "#     antenna_metrics_hourly_df = spark.sql(gn.queries.level2.antenna_metrics_hourly_query(\n",
    "#                                           table_name='table_user_metrics_df'))\n",
    "\n",
    "#     # #### antenna_interactions\n",
    "#     antenna_interactions_df = spark.sql(gn.queries.level2.antenna_interactions_query(\n",
    "#                                         table_name='table_raw_df'))\n",
    "\n",
    "#     # uncache for memory\n",
    "#     spark.catalog.clearCache()\n",
    "\n",
    "#     # #### bandicoot_metrics\n",
    "#     if att.bc_flag and not att.hdfs_flag:\n",
    "#         # join home_antenna\n",
    "#         join_cond = [bc_metrics_df.name == user_home_antenna_df.user_id]\n",
    "#         bc_metrics_df = bc_metrics_df\\\n",
    "#             .join(user_home_antenna_df, join_cond, 'inner')\\\n",
    "#             .drop('user_id', 'name', 'month')\n",
    "\n",
    "#         # keep weight as number of users adding to each antenna\n",
    "#         antenna_weight = bc_metrics_df.groupBy('antenna_id').count()\n",
    "\n",
    "#         # calculate antenna means\n",
    "#         antenna_bandicoot_features_df = bc_metrics_df.groupBy('antenna_id')\\\n",
    "#             .mean().drop('avg(antenna_id)')\n",
    "#         # averaging drops out \"delay\" columns, because they are entirely empty\n",
    "\n",
    "#         # renaming the columns for better readability\n",
    "#         clean_cols = [c.replace('avg(', '').replace(')', '') for c in\n",
    "#                       antenna_bandicoot_features_df.columns]\n",
    "#         antenna_bandicoot_features_df = antenna_bandicoot_features_df\\\n",
    "#             .toDF(*clean_cols)\n",
    "\n",
    "#         # add user weight\n",
    "#         antenna_bandicoot_features_df = antenna_bandicoot_features_df\\\n",
    "#             .join(antenna_weight, 'antenna_id', 'inner')\n",
    "\n",
    "#     # #### save final outputs\n",
    "#     antenna_metrics_week_df.coalesce(1)\\\n",
    "#         .write.csv(att.antenna_features_path+'week/'+str(i),\n",
    "#                    mode='overwrite', header=True)\n",
    "\n",
    "#     antenna_metrics_hourly_df.coalesce(1)\\\n",
    "#         .write.csv(att.antenna_features_path+'hourly/'+str(i),\n",
    "#                    mode='overwrite', header=True)\n",
    "\n",
    "#     antenna_interactions_df.coalesce(1)\\\n",
    "#         .write.csv(att.antenna_features_path+'interactions/'+str(i),\n",
    "#                    mode='overwrite', header=True)\n",
    "\n",
    "#     if att.bc_flag:\n",
    "#         antenna_bandicoot_features_df.coalesce(1)\\\n",
    "#             .write.csv(att.antenna_features_path+'bc/'+str(i),\n",
    "#                        mode='overwrite', header=True)\n",
    "#         # remove temporary bandicoot file on user level\n",
    "#         os.remove('../bandicoot_indicators_'+str(i)+'.csv')\n",
    "\n",
    "# # ### --- LOOP END ---\n",
    "\n",
    "# # collect home antennas for all users\n",
    "# home_antennas = gn.union_all(home_antennas_list)\n",
    "\n",
    "# # save home antennas for later\n",
    "# home_antennas.coalesce(1).\\\n",
    "#     write.csv('home_antennas', header=True, mode='overwrite')\n",
    "\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark environment for Part 3 created!\n"
     ]
    }
   ],
   "source": [
    "# # --- Part 3 --- (Weighted unification of antenna features & create single\n",
    "# # output file: final_features.csv )\n",
    "spark = SparkSession.builder.master(att.sparkmaster)\\\n",
    "    .appName('cdr_extraction_part3').getOrCreate()\n",
    "print('Spark environment for Part 3 created!')\n",
    "\n",
    "# ## antenna locations\n",
    "# read in table\n",
    "antennas_locations = spark.read.csv(att.antennas_file,\n",
    "                                    header=True,\n",
    "                                    inferSchema=True)\n",
    "\n",
    "# create raw table to query & cache\n",
    "antennas_locations.createOrReplaceTempView('table_antennas_locations')\n",
    "spark.catalog.cacheTable('table_antennas_locations')\n",
    "\n",
    "# ## home antennas\n",
    "home_antennas = gn.sdf_from_folder(folder=att.home_antennas_path, attributes=att,\n",
    "                                   sparksession=spark, header=True,\n",
    "                                   inferSchema=True)\n",
    "# keep number of antennas\n",
    "n_home_antennas = home_antennas.select('antenna_id').distinct().count()\n",
    "\n",
    "# ### antenna_metrics_week\n",
    "antenna_metrics_week_df = gn.aggregate_chunks(feature_type='week', attributes=att,\n",
    "                                              sparksession=spark, cache_table=False,\n",
    "                                              query=gn.queries.level2.antenna_metrics_agg_query(\n",
    "                                                        table_name='table_antenna_metrics_week_df',\n",
    "                                                        columns='week_part,week_number'))\n",
    "\n",
    "# ### antenna_metrics_hourly\n",
    "antenna_metrics_hourly_df = gn.aggregate_chunks(feature_type='hourly', attributes=att,\n",
    "                                                sparksession=spark, cache_table=False,\n",
    "                                                query=gn.queries.level2.antenna_metrics_agg_query(\n",
    "                                                        table_name='\\\n",
    "                                                        table_antenna_metrics_hourly_df',\n",
    "                                                        columns='hour'))\n",
    "\n",
    "# ### antenna_interactions\n",
    "antenna_interactions_df = gn.aggregate_chunks(feature_type='interactions', attributes=att,\n",
    "                                              sparksession=spark, cache_table=False,\n",
    "                                              query=gn.queries.level2.antenna_interactions_agg_query(\n",
    "                                                        table_name='\\\n",
    "                                                        table_antenna_metrics_interactions_df'))\n",
    "\n",
    "\n",
    "# ### alltime_features\n",
    "alltime_features_df = spark.sql(gn.queries.level3.alltime_features_query(\n",
    "                                table_name='table_antenna_metrics_week_df'))\n",
    "\n",
    "# ### active_users_features\n",
    "active_users_features_df = home_antennas.select('antenna_id', 'user_id')\\\n",
    "    .distinct().groupBy('antenna_id').count()\\\n",
    "    .selectExpr('antenna_id', 'count as active_users')\n",
    "\n",
    "# ### interaction_features\n",
    "interaction_features_df = spark.sql(gn.queries.level3.interaction_features_query(\n",
    "                                table_name='\\\n",
    "                                table_antenna_metrics_interactions_df',\n",
    "                                c_coord=att.c_coord,\n",
    "                                n_home_antennas=n_home_antennas))\n",
    "\n",
    "# ### variance_features\n",
    "variance_features_df = spark.sql(gn.queries.level3.variance_features_query(\n",
    "                                table_name='table_antenna_metrics_week_df'))\n",
    "\n",
    "# ### daily_features\n",
    "daily_features_df = spark.sql(gn.queries.level3.daily_features_query(\n",
    "                             table_name='table_antenna_metrics_week_df'))\n",
    "\n",
    "# ### hourly_features\n",
    "hourly_features_df = spark.sql(gn.queries.level3.hourly_features_query(\n",
    "                                table_name='table_antenna_metrics_hourly_df',\n",
    "                                work_day=att.work_day,\n",
    "                                early_peak=att.early_peak,\n",
    "                                late_peak=att.late_peak))\n",
    "\n",
    "# **Level 4**: Final feature table\n",
    "print('Starting with Level 4: Final feature table.')\n",
    "\n",
    "final_features_df = antennas_locations\\\n",
    "    .join(alltime_features_df, 'antenna_id', 'left')\\\n",
    "    .join(active_users_features_df, 'antenna_id', 'left')\\\n",
    "    .join(interaction_features_df, 'antenna_id', 'left')\\\n",
    "    .join(variance_features_df, 'antenna_id', 'left')\\\n",
    "    .join(daily_features_df, 'antenna_id', 'left')\\\n",
    "    .join(hourly_features_df, 'antenna_id', 'left')\n",
    "\n",
    "# ## save final output\n",
    "final_features_df.coalesce(1).write.csv('final_features', mode='overwrite',\n",
    "                                        header=True)\n",
    "if not att.hdfs_flag:\n",
    "    os.rename(glob.glob('../final_features/*.csv')[0], '../final_features.csv')\n",
    "    shutil.rmtree('../final_features')\n",
    "\n",
    "spark.stop()\n",
    "if att.clean_up and not att.hdfs_flag:\n",
    "    shutil.rmtree(attributes.antenna_features_path)\n",
    "\n",
    "print('DONE! Features file saved to final_features.csv.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
