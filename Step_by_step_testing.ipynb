{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # operating system functions like renaming files and directories\n",
    "import shutil  # recursive file and directory operations\n",
    "import glob  # pattern matching for paths\n",
    "import subprocess  # for python to interact with the HDFS\n",
    "import argparse  # entering flags from the cmd line\n",
    "import bandicoot as bc  # MIT toolkit for creating bandicoot indicators\n",
    "import pyarrow as pa  # for python to interact with the HDFS\n",
    "import gnuper as gn  # gnuper package for creating cdr features\n",
    "from pyspark.sql import SparkSession  # using spark context for big data files\n",
    "from pyspark.sql.functions import col  # needed for function over each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_flag = False\n",
    "bc_flag = False\n",
    "hdfs_flag = True\n",
    "verbose = False\n",
    "clean_up = False\n",
    "raw_data_path = 'CDR/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define attributes for this session\n",
    "att = gn.Attributes(mp_flag=mp_flag,\n",
    "                    bc_flag=bc_flag,\n",
    "                    hdfs_flag=hdfs_flag,\n",
    "                    verbose=verbose,\n",
    "                    clean_up=clean_up,\n",
    "                    raw_data_path=raw_data_path,\n",
    "                    cap_coords=[15.500654, 32.559899],  # capital gps\n",
    "                    weekend_days=[5, 6],\n",
    "                    max_chunksize=100,\n",
    "                    sparkmaster='yarn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Part 1 --- (Preprocessing of raw files and saving by user)\n",
    "spark = SparkSession.builder.master(att.sparkmaster)\\\n",
    "    .appName('cdr_extraction_part1').getOrCreate()\n",
    "print('Spark environment for Part 1 created!')\n",
    "\n",
    "# ## antennas datasets\n",
    "# read cell and antenna locations into a spark dataframe (sdf)\n",
    "raw_locations = gn.read_as_sdf(file=att.raw_locations,\n",
    "                               sparksession=spark, header=False,\n",
    "                                colnames=['cell_id', 'antenna_id',\n",
    "                                          'longitude', 'latitude'],\n",
    "                                query=gn.queries.general.raw_locations_query())\n",
    "# create raw table to query and cache it as we will query it for every day\n",
    "raw_locations.createOrReplaceTempView('table_raw_locations')\n",
    "spark.catalog.cacheTable('table_raw_locations')\n",
    "\n",
    "# solely antenna locations as sdf (= ignore cell_id)\n",
    "# FILE: save as 1 csv next to the raw data as we will need it later on\n",
    "raw_locations.selectExpr('antenna_id', 'longitude', 'latitude')\\\n",
    "    .dropDuplicates().write.csv(att.antennas_path,\n",
    "                                mode='overwrite', header=True)\n",
    "print('Antenna SDF & table created!')\n",
    "\n",
    "# ## Preprocessing\n",
    "# **Level 0**: General preprocessing of raw call detail records\n",
    "# Storing daily files in a unified dataframe\n",
    "print('Starting with Level 0: General preprocessing of raw CDRs.')\n",
    "\n",
    "# ### CDR datasets\n",
    "if att.verbose:\n",
    "    dates = gn.files_in_folder(folder=att.raw_data_path,\n",
    "                               file_pattern='20*.csv',\n",
    "                               hdfs_flag=att.hdfs_flag)\n",
    "    dates = [os.path.basename(d).replace('.csv', '') for d in dates]\n",
    "    print('Available CDR Dates: '+str(dates))  # doublechecking\n",
    "\n",
    "# order of the raw columns\n",
    "raw_colnames = ['CALL_RECORD_TYPE', 'CALLER_MSISDN', 'CALL_DATE',\n",
    "                'BASIC_SERVICE', 'MS_LOCATION', 'CALL_PARTNER_IDENTITY_TYPE',\n",
    "                'CALL_PARTNER_IDENTITY', 'TAC_CODE', 'CALL_DURATION']\n",
    "\n",
    "# reading in user_ids\n",
    "users = gn.sdf_from_folder(folder=att.raw_data_path, attributes=att,\n",
    "                           sparksession=spark, file_pattern='20*.csv',\n",
    "                           header=False, colnames=raw_colnames,\n",
    "                           query=gn.queries.general.get_user_ids_query(), action='union')\n",
    "# drop duplicate ids and create table to query\n",
    "users = users.dropDuplicates()\n",
    "users.createOrReplaceTempView('table_user_ids')\n",
    "# create chunk id for every user, based on max users per chunk\n",
    "users_w_chunk = spark.sql(gn.queries.general.chunking_query(\n",
    "                              table_name='table_user_ids',\n",
    "                              max_chunksize=att.max_chunksize))\n",
    "users_w_chunk.createOrReplaceTempView('table_chunk_ids')\n",
    "# cache this table as well due to querying it for every day\n",
    "spark.catalog.cacheTable('table_chunk_ids')\n",
    "\n",
    "# preprocess every single file and save it split by chunk id\n",
    "gn.sdf_from_folder(folder=att.raw_data_path, attributes=att,\n",
    "                   sparksession=spark, file_pattern='20*.csv',\n",
    "                   header=False,\n",
    "                   colnames=raw_colnames,\n",
    "                   query=gn.queries.level0.raw_preprocessing_query(\n",
    "                           cump=att.call_unit_multiplicator),\n",
    "                   save_path=att.chunking_path, save_format='csv',\n",
    "                   save_header=True, save_mode='append',\n",
    "                   save_partition='chunk_id', action='save')\n",
    "print('CSV files created for user chunks.')\n",
    "\n",
    "spark.stop()\n",
    "print('DONE with Part 1! User chunks saved to chunking folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Part 2 --- (create antenna indicators in a loop over the chunks)\n",
    "spark = SparkSession.builder.master(att.sparkmaster)\\\n",
    "    .appName('cdr_extraction_part2').getOrCreate()\n",
    "print('Spark environment for Part 2 created!')\n",
    "\n",
    "# ## antenna locations\n",
    "# read in table which was created in part 1\n",
    "antennas_locations = spark.read.csv(att.antennas_path,\n",
    "                                    header=True,\n",
    "                                    inferSchema=True)\n",
    "\n",
    "# create raw table to query and cache it for several queries\n",
    "antennas_locations.createOrReplaceTempView('table_antennas_locations')\n",
    "spark.catalog.cacheTable('table_antennas_locations')\n",
    "\n",
    "# ## User chunks\n",
    "if att.hdfs_flag:\n",
    "#     chunks = gn.ls_hdfs(att.chunking_path, recursive=False, pattern='*=*')\n",
    "#     chunks = [os.path.basename(p) for p in chunks]\n",
    "    (ret, out, err) = gn.run_cmd(['hdfs', 'dfs', '-ls', '-R', \n",
    "                                  att.chunking_path, \"| grep drwx | awk -F'/' '{print $NF}'\"], \n",
    "                                  shell=True)\n",
    "    chunks = out.decode('utf-8').split()\n",
    "else:\n",
    "    chunks = sorted(next(os.walk(att.chunking_path))[1])\n",
    "# save length for loop\n",
    "n = len(chunks)\n",
    "print('Number of available user chunks: '+str(n))\n",
    "\n",
    "\n",
    "# ## LOOP\n",
    "# ### --- LOOP START ---\n",
    "for chunk in chunks:\n",
    "    i = chunks.index(chunk)+1\n",
    "    print('Starting with '+chunk+', iteration '+str(i)+' out of '+str(n))\n",
    "\n",
    "    # read in chunk and filter out machines and multi-users\n",
    "    raw_df = gn.sdf_from_folder(att.chunking_path+chunk+'/', att, spark,\n",
    "                                header=True, inferSchema=False,\n",
    "                                file_pattern='*.csv',\n",
    "                                query=gn.queries.level0.filter_machines_query(\n",
    "                                max_weekly_interactions=att.\n",
    "                                max_weekly_interactions))\n",
    "\n",
    "    # quick look\n",
    "    if att.verbose and i == 1:\n",
    "        # double check file format\n",
    "        print('Raw aggregated SDF structure:')\n",
    "        raw_df.show(10)\n",
    "\n",
    "    # and register as table\n",
    "    raw_df.createOrReplaceTempView('table_raw_df')\n",
    "\n",
    "    # number of files for each save\n",
    "    n_files = min(10, round(3000/n))\n",
    "\n",
    "    # ## home antennas\n",
    "    user_home_antenna_df = spark.sql(gn.queries.level1.user_home_antenna_query(\n",
    "                                     noct_time=att.noct_time,\n",
    "                                     table_name='table_raw_df'))\n",
    "    # save as table and cache it\n",
    "    user_home_antenna_df.createOrReplaceTempView('table_user_home_antenna_df')\n",
    "    spark.catalog.cacheTable('table_user_home_antenna_df')\n",
    "    \n",
    "\n",
    "    user_home_antenna_df.coalesce(n_files)\\\n",
    "        .write.csv(att.home_antennas_path+str(i), header=True,\n",
    "                   mode='overwrite')\n",
    "\n",
    "\n",
    "    # unique users in this chunk (doublechecking)\n",
    "    n_users = raw_df.select('caller_id').distinct().count()\n",
    "    n_users_w_ha = user_home_antenna_df.select('user_id').distinct().count()\n",
    "    print('Ratio of users in this chunk with home antenna: '+str(n_users_w_ha/n_users))\n",
    "\n",
    "    # **Level 1**: Intermediate tables on user level (user_id still visible)\n",
    "\n",
    "    # Datasets created in first iteration with columns in brackets:\n",
    "    # + **user_metrics**: metrics aggregated per user_id, day and hour\n",
    "    #  *(user_id, day, hour, og_calls, ic_calls, og_sms, ic_sms,\n",
    "    #    og_vol, ic_vol)*\n",
    "    # + **user_home_antenna**: monthly estimate of antenna closest to home\n",
    "    #  location per user *(user_id, antenna_id, month)*\n",
    "    # + **user_bandicoot_features**: bandicoot interactions on user level per\n",
    "    #  month *(user_id, month, ...)*\n",
    "\n",
    "    print('Starting with Level 1: Intermediate tables on user level \\\n",
    "    (user_id still visible).')\n",
    "\n",
    "    # #### user_metrics\n",
    "    user_metrics_df = spark.sql(gn.queries.level1.user_metrics_query(\n",
    "                                table_name='table_raw_df'))\n",
    "    user_metrics_df.createOrReplaceTempView('table_user_metrics_df')\n",
    "    \n",
    "    # #### bandicoot_metrics\n",
    "    if att.bc_flag:\n",
    "\n",
    "        bc_metrics_df = spark.sql(gn.queries.level1.bc_metrics_query(\n",
    "                                  table_name='table_raw_df'))\n",
    "\n",
    "        # define unique users\n",
    "        users = [str(u.caller_id) for u in bc_metrics_df.select('caller_id')\n",
    "                 .dropDuplicates().collect()]\n",
    "\n",
    "        # save into single user folders\n",
    "        bc_metrics_df.coalesce(1).write.save(\n",
    "                    path=att.bandicoot_path,\n",
    "                    format='csv',\n",
    "                    header=True,\n",
    "                    mode='overwrite',\n",
    "                    partitionBy='caller_id')\n",
    "\n",
    "        # save antenna file in the same directory\n",
    "        antennas_locations.coalesce(1).write.save(\n",
    "            path=att.bandicoot_path+'antennas',\n",
    "            format='csv',\n",
    "            header=True,\n",
    "            mode='append')\n",
    "\n",
    "        # proper file naming and directory structure\n",
    "        if att.hdfs_flag:\n",
    "            # get all file names\n",
    "            bc_list = gn.ls_hdfs(att.bandicoot_path, '*.csv', recursive=True)\n",
    "            # connect via API\n",
    "            fs = pa.hdfs.connect()\n",
    "            # rename and move up 1 folder\n",
    "            for file in bc_list:\n",
    "                fs.rename(file, os.path.dirname(file).replace('caller_id=', '')+'.csv')\n",
    "            # create local directory\n",
    "            target_dir = os.getcwd()+'/user_bandicoot/'\n",
    "            if not os.path.exists(target_dir):\n",
    "                os.makedirs(target_dir)\n",
    "            # copy to local in order to use bandicoot functions\n",
    "            (ret, out, err) = gn.run_cmd(['hdfs', 'dfs', '-copyToLocal', '-f', \n",
    "                                         att.bandicoot_path+'*.csv', target_dir])\n",
    "\n",
    "        else:\n",
    "            # remove files from potential previous run\n",
    "            if os.path.exists(att.bandicoot_path):\n",
    "                shutil.rmtree(att.bandicoot_path)\n",
    "                \n",
    "            # get all subdirectories in bandicoot folder\n",
    "            subdirs = next(os.walk(att.bandicoot_path))[1]\n",
    "\n",
    "            for d in subdirs:\n",
    "                u = d.replace('caller_id=', '')  # extract user id\n",
    "                # rename csv to user id and move up\n",
    "                os.rename(glob.glob(att.bandicoot_path+d+'/*.csv')[0],\n",
    "                          att.bandicoot_path+u+'.csv')\n",
    "                shutil.rmtree(att.bandicoot_path+d)  # delete obsolete dir\n",
    "            print('Single User Files for bandicoot created for chunk '+str(i))\n",
    "\n",
    "        # execute bandicoot calculation as batch\n",
    "        indicators = gn.bc_batch(users, att)\n",
    "\n",
    "        # save as csv\n",
    "        bc.io.to_csv(indicators, att.bandicoot_path+'bandicoot_indicators_'+str(i)+'.csv')\n",
    "        print('Bandicoot files csvs created for chunk '+str(i))\n",
    "        \n",
    "        # copy back to hdfs for proper reading into sdf\n",
    "        if att.hdfs_flag:\n",
    "            (ret, out, err)= gn.run_cmd(['hdfs', 'dfs', '-copyFromLocal', \n",
    "                                      att.bandicoot_path+'bandicoot_indicators_'+str(i)+'.csv', \n",
    "                                      att.bandicoot_path])\n",
    "\n",
    "        # re-read as single sdf\n",
    "        bc_metrics_df = spark.read.csv(att.bandicoot_path+'bandicoot_indicators_' + str(i) +\n",
    "                                       '.csv', header=True, inferSchema=True)\n",
    "\n",
    "        # cleaning\n",
    "        bc_metrics_df = bc_metrics_df\\\n",
    "            .drop('reporting__antennas_path',\n",
    "                  'reporting__attributes_path',\n",
    "                  'reporting__recharges_path',\n",
    "                  'reporting__version',\n",
    "                  'reporting__code_signature',\n",
    "                  'reporting__groupby',\n",
    "                  'reporting__split_week',\n",
    "                  'reporting__split_day',\n",
    "                  'reporting__start_time',\n",
    "                  'reporting__end_time',\n",
    "                  'reporting__night_start',\n",
    "                  'reporting__night_end',\n",
    "                  'reporting__weekend',\n",
    "                  'reporting__number_of_antennas',\n",
    "                  'reporting__bins',\n",
    "                  'reporting__bins_with_data',\n",
    "                  'reporting__bins_without_data',\n",
    "                  'reporting__has_call',\n",
    "                  'reporting__has_text',\n",
    "                  'reporting__has_home',\n",
    "                  'reporting__has_recharges',\n",
    "                  'reporting__has_attributes',\n",
    "                  'reporting__has_network',\n",
    "                  'reporting__number_of_recharges',\n",
    "                  'reporting__percent_records_missing_location',\n",
    "                  'reporting__antennas_missing_locations',\n",
    "                  'reporting__percent_outofnetwork_calls',\n",
    "                  'reporting__percent_outofnetwork_texts',\n",
    "                  'reporting__percent_outofnetwork_contacts',\n",
    "                  'reporting__percent_outofnetwork_call_durations',\n",
    "                  'reporting__ignored_records__all',\n",
    "                  'reporting__ignored_records__interaction',\n",
    "                  'reporting__ignored_records__direction',\n",
    "                  'reporting__ignored_records__correspondent_id',\n",
    "                  'reporting__ignored_records__datetime',\n",
    "                  'reporting__ignored_records__call_duration',\n",
    "                  'reporting__ignored_records__location')\n",
    "\n",
    "    # **Level 2**: Intermediate tables on antenna level\n",
    "    # (user_id NOT visible anymore)\n",
    "\n",
    "    # Datasets created in second iteration with columns in brackets:\n",
    "    # + **antenna_interactions_generic**: alltime interactions between antennas\n",
    "    #  without allocation of home antenna locations but generic activity\n",
    "    #  *(og_antenna_id, ic_antenna_id, sms_count, calls_count, vol_sum)*\n",
    "    # + **antenna_metrics_week**: metrics aggregated per home antenna of\n",
    "    #  individual users, week and part of the week\n",
    "    #  *(antenna_id, week_part, week_number, og_calls, ic_calls, og_sms,\n",
    "    #  ic_sms, og_vol, ic_vol)*\n",
    "    # + **antenna_metrics_hourly**: metrics aggregated per home antenna of\n",
    "    #  individual users and hour\n",
    "    #  *(antenna_id, hour, og_calls, ic_calls, og_sms, ic_sms, og_vol, ic_vol)*\n",
    "    # + **antenna_interactions**: alltime interactions between antennas based\n",
    "    #  on the users' behavior to which a certain antenna is the homebase\n",
    "    #  *(antenna_id1, antenna_id2, sms_count, calls_count, vol_sum)*\n",
    "    # + **antenna_bandicoot**: alltime averaged bandicoot interactions on\n",
    "    #  antenna level *(antenna_id, ...)*\n",
    "    #\n",
    "    # A further explanation of the single features can be found\n",
    "    #  here[http://bandicoot.mit.edu/docs/reference/bandicoot.individual.html]\n",
    "\n",
    "    print('Starting with Level 2: Intermediate tables on antenna level\\\n",
    "        (user_id NOT visible anymore).')\n",
    "\n",
    "    # #### antenna_metrics_week\n",
    "    antenna_metrics_week_df = spark.sql(gn.queries.level2.antenna_metrics_week_query(\n",
    "                                    weekend_days=tuple(att.weekend_days),\n",
    "                                    table_name='table_user_metrics_df'))\n",
    "\n",
    "    # #### antenna_metrics_hourly\n",
    "    antenna_metrics_hourly_df = spark.sql(gn.queries.level2.antenna_metrics_hourly_query(\n",
    "                                      table_name='table_user_metrics_df'))\n",
    "\n",
    "    # #### antenna_interactions\n",
    "    antenna_interactions_df = spark.sql(gn.queries.level2.antenna_interactions_query(\n",
    "                                    table_name='table_raw_df'))\n",
    "\n",
    "    # uncache for memory\n",
    "    spark.catalog.clearCache()\n",
    "\n",
    "    # #### bandicoot_metrics\n",
    "    if att.bc_flag:\n",
    "        # join home_antenna\n",
    "        join_cond = [bc_metrics_df.name == user_home_antenna_df.user_id]\n",
    "        bc_metrics_df = bc_metrics_df\\\n",
    "            .join(user_home_antenna_df, join_cond, 'inner')\\\n",
    "            .drop('user_id', 'name', 'month')\n",
    "\n",
    "        # keep weight as number of users adding to each antenna\n",
    "        antenna_weight = bc_metrics_df.groupBy('antenna_id').count()\n",
    "\n",
    "        # calculate antenna means\n",
    "        antenna_bandicoot_features_df = bc_metrics_df.groupBy('antenna_id')\\\n",
    "            .mean().drop('avg(antenna_id)')\n",
    "        # averaging drops out \"delay\" columns, because they are entirely empty\n",
    "\n",
    "        # renaming the columns for better readability\n",
    "        clean_cols = [c.replace('avg(', '').replace(')', '') for c in\n",
    "                      antenna_bandicoot_features_df.columns]\n",
    "        antenna_bandicoot_features_df = antenna_bandicoot_features_df\\\n",
    "            .toDF(*clean_cols)\n",
    "\n",
    "        # add user weight\n",
    "        antenna_bandicoot_features_df = antenna_bandicoot_features_df\\\n",
    "            .join(antenna_weight, 'antenna_id', 'inner')\n",
    "\n",
    "    # #### save final outputs\n",
    "    antenna_metrics_week_df\\\n",
    "        .write.csv(att.antenna_features_path+'week/'+str(i),\n",
    "                   mode='overwrite', header=True)\n",
    "\n",
    "    antenna_metrics_hourly_df\\\n",
    "        .write.csv(att.antenna_features_path+'hourly/'+str(i),\n",
    "                   mode='overwrite', header=True)\n",
    "\n",
    "    antenna_interactions_df\\\n",
    "        .write.csv(att.antenna_features_path+'interactions/'+str(i),\n",
    "                   mode='overwrite', header=True)\n",
    "\n",
    "    if att.bc_flag:\n",
    "        antenna_bandicoot_features_df\\\n",
    "            .write.csv(att.antenna_features_path+'bc/'+str(i),\n",
    "                       mode='overwrite', header=True)\n",
    "        # remove temporary bandicoot file on user level\n",
    "        os.remove('bandicoot_indicators_'+str(i)+'.csv')\n",
    "\n",
    "    if i==n:\n",
    "        print('Done with all user chunks!')\n",
    "\n",
    "# ### --- LOOP END ---\n",
    "spark.stop()\n",
    "# delete chunking files if clean up flag is set\n",
    "if att.clean_up:\n",
    "    if att.hdfs_flag:\n",
    "        (ret, out, err)= gn.run_cmd(['hdfs', 'dfs', '-rm', '-R', att.chunking_path])\n",
    "    else:\n",
    "        shutil.rmtree(att.chunking_path)\n",
    "print('DONE! Antenna features files saved to antenna_features folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # --- Part 3 --- (Weighted unification of antenna features & create single\n",
    "# # # output file: final_features.csv )\n",
    "# spark = SparkSession.builder.master(att.sparkmaster)\\\n",
    "#     .appName('cdr_extraction_part3').getOrCreate()\n",
    "# print('Spark environment for Part 3 created!')\n",
    "\n",
    "# # ## antenna locations\n",
    "# # read in table\n",
    "# antennas_locations = spark.read.csv(att.antennas_path,\n",
    "#                                     header=True,\n",
    "#                                     inferSchema=True)\n",
    "\n",
    "# # create raw table to query & cache\n",
    "# antennas_locations.createOrReplaceTempView('table_antennas_locations')\n",
    "# spark.catalog.cacheTable('table_antennas_locations')\n",
    "\n",
    "# # ## home antennas\n",
    "# home_antennas = gn.sdf_from_folder(att.home_antennas_path, att, spark,\n",
    "#                                    recursive=True,\n",
    "#                                    header=True,\n",
    "#                                    inferSchema=True)\n",
    "# # keep number of antennas\n",
    "# n_home_antennas = home_antennas.select('antenna_id').distinct().count()\n",
    "\n",
    "# # **Level 2**: Intermediate tables on antenna level\n",
    "# # (user_id NOT visible anymore)\n",
    "\n",
    "# # Datasets created in second iteration with columns in brackets:\n",
    "# # + **antenna_metrics_week**: metrics aggregated per home antenna of individual\n",
    "# #  users, week and part of the week *(antenna_id, week_part, week_number,\n",
    "# #  og_calls, ic_calls, og_sms, ic_sms, og_vol, ic_vol)*\n",
    "# # + **antenna_metrics_hourly**: metrics aggregated per home antenna of\n",
    "# #  individual users and hour\n",
    "# #  *(antenna_id, hour, og_calls, ic_calls, og_sms, ic_sms, og_vol, ic_vol)*\n",
    "# # + **antenna_interactions**: alltime interactions between antennas based on\n",
    "# #  the users' behavior to which a certain antenna is the homebase\n",
    "# #  *(antenna_id1, antenna_id2, sms_count, calls_count, vol_sum)*\n",
    "# # + **antenna_bandicoot**: alltime averaged bandicoot interactions on\n",
    "# #  antenna level *(antenna_id, ...)*\n",
    "# #\n",
    "# # A further explanation of the single features can be found\n",
    "# #  here[http://bandicoot.mit.edu/docs/reference/bandicoot.individual.html]\n",
    "# print('Starting with Level 2: Intermediate tables on antenna level\\\n",
    "#  (user_id NOT visible anymore).')\n",
    "\n",
    "# # ### antenna_metrics_week\n",
    "# antenna_metrics_week_df = gn.aggregate_chunks(feature_type='week', attributes=att,\n",
    "#                                               sparksession=spark, cache_table=False,\n",
    "#                                               query=gn.queries.level2.antenna_metrics_agg_query(\n",
    "#                                                         table_name='table_antenna_metrics_week_df',\n",
    "#                                                         columns='week_part,week_number'))\n",
    "\n",
    "# # ### antenna_metrics_hourly\n",
    "# antenna_metrics_hourly_df = gn.aggregate_chunks(feature_type='hourly', attributes=att,\n",
    "#                                                 sparksession=spark, cache_table=False,\n",
    "#                                                 query=gn.queries.level2.antenna_metrics_agg_query(\n",
    "#                                                         table_name='\\\n",
    "#                                                         table_antenna_metrics_hourly_df',\n",
    "#                                                         columns='hour'))\n",
    "\n",
    "# # ### antenna_interactions\n",
    "# antenna_interactions_df = gn.aggregate_chunks(feature_type='interactions', attributes=att,\n",
    "#                                               sparksession=spark, cache_table=False,\n",
    "#                                               query=gn.queries.level2.antenna_interactions_agg_query(\n",
    "#                                                         table_name='\\\n",
    "#                                                         table_antenna_metrics_interactions_df'))\n",
    "\n",
    "# # ### bandicoot_features\n",
    "# if att.bc_flag and not att.hdfs_flag:\n",
    "#     # load in all items\n",
    "#     antenna_bandicoot_df = gn.sdf_from_folder(att.antenna_features_path+'bc/',\n",
    "#                                               attributes=att,\n",
    "#                                               sparksession=spark,\n",
    "#                                               recursive=True)\n",
    "\n",
    "#     # get total sum of weights (should be equal to total number of users!)\n",
    "#     sum_weights = antenna_bandicoot_df.select('count').groupBy().sum()\\\n",
    "#         .collect()[0][0]\n",
    "\n",
    "#     # apply weighing (multiply with weight and divide by all weights)\n",
    "#     antenna_bandicoot_df = antenna_bandicoot_df\\\n",
    "#         .select('antenna_id', *[(col(col_name)*col('count')/sum_weights)\n",
    "#                                 .alias(col_name) for col_name in\n",
    "#                                 antenna_bandicoot_df.columns[1:]])\n",
    "\n",
    "#     antenna_bandicoot_df = antenna_bandicoot_df.groupBy('antenna_id').\\\n",
    "#         sum().drop('sum(antenna_id)', 'sum(count)')  # sum up per antenna_id\n",
    "\n",
    "#     # renaming to more readable column names\n",
    "#     clean_cols = [c.replace('sum(', '').replace(')', '')\n",
    "#                   for c in antenna_bandicoot_df.columns]\n",
    "#     antenna_bandicoot_df = antenna_bandicoot_df.toDF(*clean_cols)\n",
    "\n",
    "#     print('Aggregated chunks for bandicoot!')\n",
    "\n",
    "# # **Level 3**: Intermediate feature tables\n",
    "# #\n",
    "# # Datasets holding different features/variables with columns in brackets:\n",
    "# # + **alltime_features**: *(antenna_id, calls_ratio, sms_ratio, vol_ratio,\n",
    "# #  sms2calls_ratio)*\n",
    "# # + **active_users_features**: *(antenna_id, active_users)*\n",
    "# # + **interaction_features**: *(antenna_id, calls_dist_mean, sms_dist_mean,\n",
    "# #  calls_isolation, sms_isolation, calls_entropy, sms_entropy, dist2c)*\n",
    "# # + **variance_features**: *(antenna_id, calls_ratio_var, sms_ratio_var,\n",
    "# #  vol_ratio_var)*\n",
    "# # + **daily_features**: *(antenna_id, og_calls_week_ratio, og_sms_week_ratio,\n",
    "# #  og_vol_week_ratio, ic_calls_week_ratio, ic_sms_week_ratio,\n",
    "# #  ic_vol_week_ratio)*\n",
    "# # + **hourly_features**: *(antenna_id, og_calls_work_ratio, og_sms_work_ratio,\n",
    "# #  og_vol_work_ratio, ic_calls_work_ratio, ic_sms_work_ratio,\n",
    "# #  ic_vol_work_ratio, og_calls_peak_ratio, og_sms_peak_ratio,\n",
    "# #  og_vol_peak_ratio, ic_calls_peak_ratio, ic_sms_peak_ratio,\n",
    "# #  ic_vol_peak_ratio)*\n",
    "# # + **antenna_bandicoot_features**: alltime averaged bandicoot interactions on\n",
    "# #  antenna level *(antenna_id, ...)*\n",
    "# #\n",
    "# # A further explanation of the single features can be found\n",
    "# #  here[http://bandicoot.mit.edu/docs/reference/bandicoot.individual.html]\n",
    "# print('Starting with Level 3: Intermediate feature tables.')\n",
    "# # ### alltime_features\n",
    "# alltime_features_df = spark.sql(gn.queries.level3.alltime_features_query(\n",
    "#                                 table_name='table_antenna_metrics_week_df'))\n",
    "\n",
    "# # ### active_users_features\n",
    "# active_users_features_df = home_antennas.select('antenna_id', 'user_id')\\\n",
    "#     .distinct().groupBy('antenna_id').count()\\\n",
    "#     .selectExpr('antenna_id', 'count as active_users')\n",
    "\n",
    "# # ### interaction_features\n",
    "# interaction_features_df = spark.sql(gn.queries.level3.interaction_features_query(\n",
    "#                                 table_name='\\\n",
    "#                                 table_antenna_metrics_interactions_df',\n",
    "#                                 c_coord=att.c_coord,\n",
    "#                                 n_home_antennas=n_home_antennas))\n",
    "\n",
    "# # ### variance_features\n",
    "# variance_features_df = spark.sql(gn.queries.level3.variance_features_query(\n",
    "#                                 table_name='table_antenna_metrics_week_df'))\n",
    "\n",
    "# # ### daily_features\n",
    "# daily_features_df = spark.sql(gn.queries.level3.daily_features_query(\n",
    "#                              table_name='table_antenna_metrics_week_df'))\n",
    "\n",
    "# # ### hourly_features\n",
    "# hourly_features_df = spark.sql(gn.queries.level3.hourly_features_query(\n",
    "#                                 table_name='table_antenna_metrics_hourly_df',\n",
    "#                                 work_day=att.work_day,\n",
    "#                                 early_peak=att.early_peak,\n",
    "#                                 late_peak=att.late_peak))\n",
    "\n",
    "# # **Level 4**: Final feature table\n",
    "# print('Starting with Level 4: Final feature table.')\n",
    "\n",
    "# final_features_df = antennas_locations\\\n",
    "#     .join(alltime_features_df, 'antenna_id', 'left')\\\n",
    "#     .join(active_users_features_df, 'antenna_id', 'left')\\\n",
    "#     .join(interaction_features_df, 'antenna_id', 'left')\\\n",
    "#     .join(variance_features_df, 'antenna_id', 'left')\\\n",
    "#     .join(daily_features_df, 'antenna_id', 'left')\\\n",
    "#     .join(hourly_features_df, 'antenna_id', 'left')\n",
    "\n",
    "# if att.bc_flag and not att.hdfs_flag:\n",
    "#     final_features_df = final_features_df.join(antenna_bandicoot_df,\n",
    "#                                                'antenna_id', 'left')\n",
    "\n",
    "# # ## save final output\n",
    "# final_features_df.coalesce(1).write.csv('final_features', mode='overwrite',\n",
    "#                                         header=True)\n",
    "# # rename final file if local\n",
    "# if not att.hdfs_flag:\n",
    "#     os.rename(glob.glob('final_features/*.csv')[0], 'final_features.csv')\n",
    "#     shutil.rmtree('final_features')\n",
    "\n",
    "# spark.stop()\n",
    "# if att.clean_up:\n",
    "#     if att.hdfs_flag:\n",
    "#             (ret, out, err)= gn.run_cmd(['hdfs', 'dfs', '-rm', '-R', att.antenna_features_path])\n",
    "#     else:\n",
    "#         shutil.rmtree(att.antenna_features_path)\n",
    "\n",
    "# print('DONE! Features file saved to final_features/.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
