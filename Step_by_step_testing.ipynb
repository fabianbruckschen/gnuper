{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # operating system functions like renaming files and directories\n",
    "import shutil  # recursive file and directory operations\n",
    "import glob  # pattern matching for paths\n",
    "import argparse  # entering flags from the cmd line\n",
    "import bandicoot as bc  # MIT toolkit for creating bandicoot indicators\n",
    "import pyarrow as pa  # for python to interact with the HDFS\n",
    "import gnuper as gn  # gnuper package for creating cdr features\n",
    "from pyspark.sql import SparkSession  # using spark context for big data files\n",
    "from pyspark.sql.functions import col  # needed for function over each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_flag = True\n",
    "bc_flag = False\n",
    "hdfs_flag = True\n",
    "verbose = True\n",
    "clean_up = False\n",
    "raw_data_path = 'CDR/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define attributes for this session\n",
    "att = gn.Attributes(mp_flag=mp_flag,\n",
    "                    bc_flag=bc_flag,\n",
    "                    hdfs_flag=hdfs_flag,\n",
    "                    verbose=verbose,\n",
    "                    clean_up=clean_up,\n",
    "                    raw_data_path=raw_data_path,\n",
    "                    cap_coords=[15.500654, 32.559899],  # capital gps\n",
    "                    weekend_days=[5, 6],\n",
    "                    max_chunksize=2000,\n",
    "                    sparkmaster='yarn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # --- Part 1 --- (Preprocessing of raw files and saving by user)\n",
    "# spark = SparkSession.builder.master(att.sparkmaster)\\\n",
    "#     .appName('cdr_extraction_part1').getOrCreate()\n",
    "# print('Spark environment for Part 1 created!')\n",
    "\n",
    "# # ## antennas datasets\n",
    "# # read cell and antenna locations into a spark dataframe (sdf)\n",
    "# raw_locations = gn.read_as_sdf(file=att.raw_locations,\n",
    "#                                sparksession=spark, header=False,\n",
    "#                                 colnames=['cell_id', 'antenna_id',\n",
    "#                                           'longitude', 'latitude'],\n",
    "#                                 query=gn.queries.general.raw_locations_query())\n",
    "# # create raw table to query and cache it as we will query it for every day\n",
    "# raw_locations.createOrReplaceTempView('table_raw_locations')\n",
    "# spark.catalog.cacheTable('table_raw_locations')\n",
    "\n",
    "# # solely antenna locations as sdf (= ignore cell_id)\n",
    "# # FILE: save as 1 csv next to the raw data as we will need it later on\n",
    "# raw_locations.selectExpr('antenna_id', 'longitude', 'latitude')\\\n",
    "#     .dropDuplicates().write.csv(att.antennas_path,\n",
    "#                                 mode='overwrite', header=True)\n",
    "# print('Antenna SDF & table created!')\n",
    "\n",
    "# # ## Preprocessing\n",
    "# # **Level 0**: General preprocessing of raw call detail records\n",
    "# # Storing daily files in a unified dataframe\n",
    "# print('Starting with Level 0: General preprocessing of raw CDRs.')\n",
    "\n",
    "# # ### CDR datasets\n",
    "# if att.verbose:\n",
    "#     dates = gn.files_in_folder(folder=att.raw_data_path,\n",
    "#                                file_pattern='20*.csv',\n",
    "#                                hdfs_flag=att.hdfs_flag)\n",
    "#     dates = [os.path.basename(d).replace('.csv', '') for d in dates]\n",
    "#     print('Available CDR Dates: '+str(dates))  # doublechecking\n",
    "\n",
    "# # order of the raw columns\n",
    "# raw_colnames = ['CALL_RECORD_TYPE', 'CALLER_MSISDN', 'CALL_DATE',\n",
    "#                 'BASIC_SERVICE', 'MS_LOCATION', 'CALL_PARTNER_IDENTITY_TYPE',\n",
    "#                 'CALL_PARTNER_IDENTITY', 'TAC_CODE', 'CALL_DURATION']\n",
    "\n",
    "# # reading in user_ids\n",
    "# users = gn.sdf_from_folder(folder=att.raw_data_path, attributes=att,\n",
    "#                            sparksession=spark, file_pattern='20*.csv',\n",
    "#                            header=False, colnames=raw_colnames,\n",
    "#                            query=gn.queries.general.get_user_ids_query(), action='union')\n",
    "# # drop duplicate ids and create table to query\n",
    "# users = users.dropDuplicates()\n",
    "# users.createOrReplaceTempView('table_user_ids')\n",
    "# # create chunk id for every user, based on max users per chunk\n",
    "# users_w_chunk = spark.sql(gn.queries.general.chunking_query(\n",
    "#                               table_name='table_user_ids',\n",
    "#                               max_chunksize=att.max_chunksize))\n",
    "# users_w_chunk.createOrReplaceTempView('table_chunk_ids')\n",
    "# # cache this table as well due to querying it for every day\n",
    "# spark.catalog.cacheTable('table_chunk_ids')\n",
    "\n",
    "# # preprocess every single file and save it split by chunk id\n",
    "# gn.sdf_from_folder(folder=att.raw_data_path, attributes=att,\n",
    "#                    sparksession=spark, file_pattern='20*.csv',\n",
    "#                    header=False,\n",
    "#                    colnames=raw_colnames,\n",
    "#                    query=gn.queries.level0.raw_preprocessing_query(\n",
    "#                            cump=att.call_unit_multiplicator),\n",
    "#                    save_path=att.chunking_path, save_format='csv',\n",
    "#                    save_header=True, save_mode='append',\n",
    "#                    save_partition='chunk_id', action='save')\n",
    "# print('CSV files created for user chunks.')\n",
    "\n",
    "# spark.stop()\n",
    "# print('DONE with Part 1! User chunks saved to chunking folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark environment for Part 2 created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read Files:   0%|          | 0/232 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available user chunks: 3\n",
      "Starting with chunk_id=1, iteration 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read Files: 100%|██████████| 232/232 [04:34<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been read and unioned!\n",
      "Raw aggregated SDF structure:\n",
      "+--------------+-----------+---------+--------+----------------+--------------------+-------------+----------+\n",
      "|     caller_id|interaction|direction|national|correspondent_id|            datetime|call_duration|antenna_id|\n",
      "+--------------+-----------+---------+--------+----------------+--------------------+-------------+----------+\n",
      "|3.9391251033E8|       text|       in|       2|    2.05923851E8|2018-10-29T23:17:...|         null|      1184|\n",
      "|3.9391251033E8|       text|       in|       2|    2.05923851E8|2018-10-29T23:17:...|         null|       613|\n",
      "|3.9391251033E8|       text|       in|       2|    2.05923851E8|2018-10-29T23:17:...|         null|      1386|\n",
      "| 3.203033252E8|       call|       in|       2|  1.7582231031E8|2018-10-29T10:20:...|           60|      1184|\n",
      "| 3.203033252E8|       call|       in|       2|  1.7582231031E8|2018-10-29T10:20:...|           60|       613|\n",
      "| 3.203033252E8|       call|       in|       2|  1.7582231031E8|2018-10-29T10:20:...|           60|      1386|\n",
      "|3.2939761774E8|       call|       in|       2|  3.3361480019E8|2018-10-29T19:02:...|           54|      1184|\n",
      "|3.2939761774E8|       call|       in|       2|  3.3361480019E8|2018-10-29T19:02:...|           54|       613|\n",
      "|3.2939761774E8|       call|       in|       2|  3.3361480019E8|2018-10-29T19:02:...|           54|      1386|\n",
      "|2.1278203754E8|       call|       in|       2|  2.2145733058E8|2018-10-29T07:21:...|           60|      1184|\n",
      "+--------------+-----------+---------+--------+----------------+--------------------+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Ratio of users in this chunk with home antenna: 0.9689223057644111\n",
      "Starting with Level 1: Intermediate tables on user level     (user_id still visible).\n",
      "Starting with Level 2: Intermediate tables on antenna level        (user_id NOT visible anymore).\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3311.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:196)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:656)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job 2788 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:932)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:930)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:930)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2126)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2039)\n\tat org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1948)\n\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1903)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)\n\t... 33 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f347e380e0e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0mantenna_metrics_hourly_df\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         .write.csv(att.antenna_features_path+'hourly/'+str(i),\n\u001b[0;32m--> 281\u001b[0;31m                    mode='overwrite', header=True)\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mantenna_interactions_df\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue)\u001b[0m\n\u001b[1;32m    927\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                        encoding=encoding, emptyValue=emptyValue)\n\u001b[0;32m--> 929\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3311.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:196)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:656)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job 2788 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:932)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:930)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:930)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2126)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2039)\n\tat org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1948)\n\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1903)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)\n\t... 33 more\n"
     ]
    }
   ],
   "source": [
    "# # --- Part 2 --- (create antenna indicators in a loop over the chunks)\n",
    "spark = SparkSession.builder.master(att.sparkmaster)\\\n",
    "    .appName('cdr_extraction_part2').getOrCreate()\n",
    "print('Spark environment for Part 2 created!')\n",
    "\n",
    "# ## antenna locations\n",
    "# read in table which was created in part 1\n",
    "antennas_locations = spark.read.csv(att.antennas_path,\n",
    "                                    header=True,\n",
    "                                    inferSchema=True)\n",
    "\n",
    "# create raw table to query and cache it for several queries\n",
    "antennas_locations.createOrReplaceTempView('table_antennas_locations')\n",
    "spark.catalog.cacheTable('table_antennas_locations')\n",
    "\n",
    "# ## User chunks\n",
    "if att.hdfs_flag:\n",
    "    chunks = gn.ls_hdfs(att.chunking_path, recursive=False, pattern='*=*')\n",
    "    chunks = [os.path.basename(p) for p in chunks]\n",
    "else:\n",
    "    chunks = sorted(next(os.walk(att.chunking_path))[1])\n",
    "# save length for loop\n",
    "n = len(chunks)\n",
    "print('Number of available user chunks: '+str(n))\n",
    "\n",
    "\n",
    "# ## LOOP\n",
    "# ### --- LOOP START ---\n",
    "for chunk in chunks:\n",
    "    i = chunks.index(chunk)+1\n",
    "    print('Starting with '+chunk+', iteration '+str(i)+' out of '+str(n))\n",
    "\n",
    "    # read in chunk and filter out machines and multi-users\n",
    "    raw_df = gn.sdf_from_folder(att.chunking_path+chunk+'/', att, spark,\n",
    "                                header=True, inferSchema=False,\n",
    "                                file_pattern='*.csv',\n",
    "                                query=gn.queries.level0.filter_machines_query(\n",
    "                                max_weekly_interactions=att.\n",
    "                                max_weekly_interactions))\n",
    "\n",
    "    # quick look\n",
    "    if att.verbose and i == 1:\n",
    "        # double check file format\n",
    "        print('Raw aggregated SDF structure:')\n",
    "        raw_df.show(10)\n",
    "\n",
    "    # and register as table\n",
    "    raw_df.createOrReplaceTempView('table_raw_df')\n",
    "\n",
    "    # number of files for each save\n",
    "#     n_files = 20\n",
    "\n",
    "    # ## home antennas\n",
    "    user_home_antenna_df = spark.sql(gn.queries.level1.user_home_antenna_query(\n",
    "                                     noct_time=att.noct_time,\n",
    "                                     table_name='table_raw_df'))\n",
    "    # save as table and cache it\n",
    "    user_home_antenna_df.createOrReplaceTempView('table_user_home_antenna_df')\n",
    "    spark.catalog.cacheTable('table_user_home_antenna_df')\n",
    "    \n",
    "\n",
    "    user_home_antenna_df.\\\n",
    "        .write.csv(att.home_antennas_path+str(i), header=True,\n",
    "                   mode='overwrite')\n",
    "\n",
    "\n",
    "    # unique users in this chunk (doublechecking)\n",
    "    n_users = raw_df.select('caller_id').distinct().count()\n",
    "    n_users_w_ha = user_home_antenna_df.select('user_id').distinct().count()\n",
    "    print('Ratio of users in this chunk with home antenna: '+str(n_users_w_ha/n_users))\n",
    "\n",
    "    # **Level 1**: Intermediate tables on user level (user_id still visible)\n",
    "\n",
    "    # Datasets created in first iteration with columns in brackets:\n",
    "    # + **user_metrics**: metrics aggregated per user_id, day and hour\n",
    "    #  *(user_id, day, hour, og_calls, ic_calls, og_sms, ic_sms,\n",
    "    #    og_vol, ic_vol)*\n",
    "    # + **user_home_antenna**: monthly estimate of antenna closest to home\n",
    "    #  location per user *(user_id, antenna_id, month)*\n",
    "    # + **user_bandicoot_features**: bandicoot interactions on user level per\n",
    "    #  month *(user_id, month, ...)*\n",
    "\n",
    "    print('Starting with Level 1: Intermediate tables on user level \\\n",
    "    (user_id still visible).')\n",
    "\n",
    "    # #### user_metrics\n",
    "    user_metrics_df = spark.sql(gn.queries.level1.user_metrics_query(\n",
    "                                table_name='table_raw_df'))\n",
    "    user_metrics_df.createOrReplaceTempView('table_user_metrics_df')\n",
    "    \n",
    "    # #### bandicoot_metrics\n",
    "    if att.bc_flag:\n",
    "\n",
    "        bc_metrics_df = spark.sql(gn.queries.level1.bc_metrics_query(\n",
    "                                  table_name='table_raw_df'))\n",
    "\n",
    "        # define unique users\n",
    "        users = [str(u.caller_id) for u in bc_metrics_df.select('caller_id')\n",
    "                 .dropDuplicates().collect()]\n",
    "\n",
    "        # save into single user folders\n",
    "        bc_metrics_df.coalesce(1).write.save(\n",
    "                    path=att.bandicoot_path,\n",
    "                    format='csv',\n",
    "                    header=True,\n",
    "                    mode='overwrite',\n",
    "                    partitionBy='caller_id')\n",
    "\n",
    "        # save antenna file in the same directory\n",
    "        antennas_locations.coalesce(1).write.save(\n",
    "            path=att.bandicoot_path+'antennas',\n",
    "            format='csv',\n",
    "            header=True,\n",
    "            mode='append')\n",
    "\n",
    "        # proper file naming and directory structure\n",
    "        if att.hdfs_flag:\n",
    "            # get all file names\n",
    "            bc_list = gn.ls_hdfs(att.bandicoot_path, '*.csv', recursive=True)\n",
    "            # connect via API\n",
    "            fs = pa.hdfs.connect()\n",
    "            # rename and move up 1 folder\n",
    "            for file in bc_list:\n",
    "                fs.rename(file, os.path.dirname(file).replace('caller_id=', '')+'.csv')\n",
    "            # create local directory\n",
    "            target_dir = os.getcwd()+'/user_bandicoot/'\n",
    "            if not os.path.exists(target_dir):\n",
    "                os.makedirs(target_dir)\n",
    "            # copy to local in order to use bandicoot functions\n",
    "            (ret, out, err) = gn.run_cmd(['hdfs', 'dfs', '-copyToLocal', '-f', \n",
    "                                         att.bandicoot_path+'*.csv', target_dir])\n",
    "\n",
    "        else:\n",
    "            # remove files from potential previous run\n",
    "            if os.path.exists(att.bandicoot_path):\n",
    "                shutil.rmtree(att.bandicoot_path)\n",
    "                \n",
    "            # get all subdirectories in bandicoot folder\n",
    "            subdirs = next(os.walk(att.bandicoot_path))[1]\n",
    "\n",
    "            for d in subdirs:\n",
    "                u = d.replace('caller_id=', '')  # extract user id\n",
    "                # rename csv to user id and move up\n",
    "                os.rename(glob.glob(att.bandicoot_path+d+'/*.csv')[0],\n",
    "                          att.bandicoot_path+u+'.csv')\n",
    "                shutil.rmtree(att.bandicoot_path+d)  # delete obsolete dir\n",
    "            print('Single User Files for bandicoot created for chunk '+str(i))\n",
    "\n",
    "        # execute bandicoot calculation as batch\n",
    "        indicators = gn.bc_batch(users, att)\n",
    "\n",
    "        # save as csv\n",
    "        bc.io.to_csv(indicators, att.bandicoot_path+'bandicoot_indicators_'+str(i)+'.csv')\n",
    "        print('Bandicoot files csvs created for chunk '+str(i))\n",
    "        \n",
    "        # copy back to hdfs for proper reading into sdf\n",
    "        if att.hdfs_flag:\n",
    "            (ret, out, err)= gn.run_cmd(['hdfs', 'dfs', '-copyFromLocal', \n",
    "                                      att.bandicoot_path+'bandicoot_indicators_'+str(i)+'.csv', \n",
    "                                      att.bandicoot_path])\n",
    "\n",
    "        # re-read as single sdf\n",
    "        bc_metrics_df = spark.read.csv(att.bandicoot_path+'bandicoot_indicators_' + str(i) +\n",
    "                                       '.csv', header=True, inferSchema=True)\n",
    "\n",
    "        # cleaning\n",
    "        bc_metrics_df = bc_metrics_df\\\n",
    "            .drop('reporting__antennas_path',\n",
    "                  'reporting__attributes_path',\n",
    "                  'reporting__recharges_path',\n",
    "                  'reporting__version',\n",
    "                  'reporting__code_signature',\n",
    "                  'reporting__groupby',\n",
    "                  'reporting__split_week',\n",
    "                  'reporting__split_day',\n",
    "                  'reporting__start_time',\n",
    "                  'reporting__end_time',\n",
    "                  'reporting__night_start',\n",
    "                  'reporting__night_end',\n",
    "                  'reporting__weekend',\n",
    "                  'reporting__number_of_antennas',\n",
    "                  'reporting__bins',\n",
    "                  'reporting__bins_with_data',\n",
    "                  'reporting__bins_without_data',\n",
    "                  'reporting__has_call',\n",
    "                  'reporting__has_text',\n",
    "                  'reporting__has_home',\n",
    "                  'reporting__has_recharges',\n",
    "                  'reporting__has_attributes',\n",
    "                  'reporting__has_network',\n",
    "                  'reporting__number_of_recharges',\n",
    "                  'reporting__percent_records_missing_location',\n",
    "                  'reporting__antennas_missing_locations',\n",
    "                  'reporting__percent_outofnetwork_calls',\n",
    "                  'reporting__percent_outofnetwork_texts',\n",
    "                  'reporting__percent_outofnetwork_contacts',\n",
    "                  'reporting__percent_outofnetwork_call_durations',\n",
    "                  'reporting__ignored_records__all',\n",
    "                  'reporting__ignored_records__interaction',\n",
    "                  'reporting__ignored_records__direction',\n",
    "                  'reporting__ignored_records__correspondent_id',\n",
    "                  'reporting__ignored_records__datetime',\n",
    "                  'reporting__ignored_records__call_duration',\n",
    "                  'reporting__ignored_records__location')\n",
    "\n",
    "    # **Level 2**: Intermediate tables on antenna level\n",
    "    # (user_id NOT visible anymore)\n",
    "\n",
    "    # Datasets created in second iteration with columns in brackets:\n",
    "    # + **antenna_interactions_generic**: alltime interactions between antennas\n",
    "    #  without allocation of home antenna locations but generic activity\n",
    "    #  *(og_antenna_id, ic_antenna_id, sms_count, calls_count, vol_sum)*\n",
    "    # + **antenna_metrics_week**: metrics aggregated per home antenna of\n",
    "    #  individual users, week and part of the week\n",
    "    #  *(antenna_id, week_part, week_number, og_calls, ic_calls, og_sms,\n",
    "    #  ic_sms, og_vol, ic_vol)*\n",
    "    # + **antenna_metrics_hourly**: metrics aggregated per home antenna of\n",
    "    #  individual users and hour\n",
    "    #  *(antenna_id, hour, og_calls, ic_calls, og_sms, ic_sms, og_vol, ic_vol)*\n",
    "    # + **antenna_interactions**: alltime interactions between antennas based\n",
    "    #  on the users' behavior to which a certain antenna is the homebase\n",
    "    #  *(antenna_id1, antenna_id2, sms_count, calls_count, vol_sum)*\n",
    "    # + **antenna_bandicoot**: alltime averaged bandicoot interactions on\n",
    "    #  antenna level *(antenna_id, ...)*\n",
    "    #\n",
    "    # A further explanation of the single features can be found\n",
    "    #  here[http://bandicoot.mit.edu/docs/reference/bandicoot.individual.html]\n",
    "\n",
    "    print('Starting with Level 2: Intermediate tables on antenna level\\\n",
    "        (user_id NOT visible anymore).')\n",
    "\n",
    "    # #### antenna_metrics_week\n",
    "    antenna_metrics_week_df = spark.sql(gn.queries.level2.antenna_metrics_week_query(\n",
    "                                    weekend_days=tuple(att.weekend_days),\n",
    "                                    table_name='table_user_metrics_df'))\n",
    "\n",
    "    # #### antenna_metrics_hourly\n",
    "    antenna_metrics_hourly_df = spark.sql(gn.queries.level2.antenna_metrics_hourly_query(\n",
    "                                      table_name='table_user_metrics_df'))\n",
    "\n",
    "    # #### antenna_interactions\n",
    "    antenna_interactions_df = spark.sql(gn.queries.level2.antenna_interactions_query(\n",
    "                                    table_name='table_raw_df'))\n",
    "\n",
    "    # uncache for memory\n",
    "    spark.catalog.clearCache()\n",
    "\n",
    "    # #### bandicoot_metrics\n",
    "    if att.bc_flag:\n",
    "        # join home_antenna\n",
    "        join_cond = [bc_metrics_df.name == user_home_antenna_df.user_id]\n",
    "        bc_metrics_df = bc_metrics_df\\\n",
    "            .join(user_home_antenna_df, join_cond, 'inner')\\\n",
    "            .drop('user_id', 'name', 'month')\n",
    "\n",
    "        # keep weight as number of users adding to each antenna\n",
    "        antenna_weight = bc_metrics_df.groupBy('antenna_id').count()\n",
    "\n",
    "        # calculate antenna means\n",
    "        antenna_bandicoot_features_df = bc_metrics_df.groupBy('antenna_id')\\\n",
    "            .mean().drop('avg(antenna_id)')\n",
    "        # averaging drops out \"delay\" columns, because they are entirely empty\n",
    "\n",
    "        # renaming the columns for better readability\n",
    "        clean_cols = [c.replace('avg(', '').replace(')', '') for c in\n",
    "                      antenna_bandicoot_features_df.columns]\n",
    "        antenna_bandicoot_features_df = antenna_bandicoot_features_df\\\n",
    "            .toDF(*clean_cols)\n",
    "\n",
    "        # add user weight\n",
    "        antenna_bandicoot_features_df = antenna_bandicoot_features_df\\\n",
    "            .join(antenna_weight, 'antenna_id', 'inner')\n",
    "\n",
    "    # #### save final outputs\n",
    "    antenna_metrics_week_df\\\n",
    "        .write.csv(att.antenna_features_path+'week/'+str(i),\n",
    "                   mode='overwrite', header=True)\n",
    "\n",
    "    antenna_metrics_hourly_df\\\n",
    "        .write.csv(att.antenna_features_path+'hourly/'+str(i),\n",
    "                   mode='overwrite', header=True)\n",
    "\n",
    "    antenna_interactions_df\\\n",
    "        .write.csv(att.antenna_features_path+'interactions/'+str(i),\n",
    "                   mode='overwrite', header=True)\n",
    "\n",
    "    if att.bc_flag:\n",
    "        antenna_bandicoot_features_df\\\n",
    "            .write.csv(att.antenna_features_path+'bc/'+str(i),\n",
    "                       mode='overwrite', header=True)\n",
    "        # remove temporary bandicoot file on user level\n",
    "        os.remove('bandicoot_indicators_'+str(i)+'.csv')\n",
    "\n",
    "    if i==n:\n",
    "        print('Done with all user chunks!')\n",
    "\n",
    "# ### --- LOOP END ---\n",
    "spark.stop()\n",
    "# delete chunking files if clean up flag is set\n",
    "if att.clean_up:\n",
    "    if att.hdfs_flag:\n",
    "        (ret, out, err)= gn.run_cmd(['hdfs', 'dfs', '-rm', '-R', att.chunking_path])\n",
    "    else:\n",
    "        shutil.rmtree(att.chunking_path)\n",
    "print('DONE! Antenna features files saved to antenna_features folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # --- Part 3 --- (Weighted unification of antenna features & create single\n",
    "# # # output file: final_features.csv )\n",
    "# spark = SparkSession.builder.master(att.sparkmaster)\\\n",
    "#     .appName('cdr_extraction_part3').getOrCreate()\n",
    "# print('Spark environment for Part 3 created!')\n",
    "\n",
    "# # ## antenna locations\n",
    "# # read in table\n",
    "# antennas_locations = spark.read.csv(att.antennas_path,\n",
    "#                                     header=True,\n",
    "#                                     inferSchema=True)\n",
    "\n",
    "# # create raw table to query & cache\n",
    "# antennas_locations.createOrReplaceTempView('table_antennas_locations')\n",
    "# spark.catalog.cacheTable('table_antennas_locations')\n",
    "\n",
    "# # ## home antennas\n",
    "# home_antennas = gn.sdf_from_folder(att.home_antennas_path, att, spark,\n",
    "#                                    recursive=True,\n",
    "#                                    header=True,\n",
    "#                                    inferSchema=True)\n",
    "# # keep number of antennas\n",
    "# n_home_antennas = home_antennas.select('antenna_id').distinct().count()\n",
    "\n",
    "# # **Level 2**: Intermediate tables on antenna level\n",
    "# # (user_id NOT visible anymore)\n",
    "\n",
    "# # Datasets created in second iteration with columns in brackets:\n",
    "# # + **antenna_metrics_week**: metrics aggregated per home antenna of individual\n",
    "# #  users, week and part of the week *(antenna_id, week_part, week_number,\n",
    "# #  og_calls, ic_calls, og_sms, ic_sms, og_vol, ic_vol)*\n",
    "# # + **antenna_metrics_hourly**: metrics aggregated per home antenna of\n",
    "# #  individual users and hour\n",
    "# #  *(antenna_id, hour, og_calls, ic_calls, og_sms, ic_sms, og_vol, ic_vol)*\n",
    "# # + **antenna_interactions**: alltime interactions between antennas based on\n",
    "# #  the users' behavior to which a certain antenna is the homebase\n",
    "# #  *(antenna_id1, antenna_id2, sms_count, calls_count, vol_sum)*\n",
    "# # + **antenna_bandicoot**: alltime averaged bandicoot interactions on\n",
    "# #  antenna level *(antenna_id, ...)*\n",
    "# #\n",
    "# # A further explanation of the single features can be found\n",
    "# #  here[http://bandicoot.mit.edu/docs/reference/bandicoot.individual.html]\n",
    "# print('Starting with Level 2: Intermediate tables on antenna level\\\n",
    "#  (user_id NOT visible anymore).')\n",
    "\n",
    "# # ### antenna_metrics_week\n",
    "# antenna_metrics_week_df = gn.aggregate_chunks(feature_type='week', attributes=att,\n",
    "#                                               sparksession=spark, cache_table=False,\n",
    "#                                               query=gn.queries.level2.antenna_metrics_agg_query(\n",
    "#                                                         table_name='table_antenna_metrics_week_df',\n",
    "#                                                         columns='week_part,week_number'))\n",
    "\n",
    "# # ### antenna_metrics_hourly\n",
    "# antenna_metrics_hourly_df = gn.aggregate_chunks(feature_type='hourly', attributes=att,\n",
    "#                                                 sparksession=spark, cache_table=False,\n",
    "#                                                 query=gn.queries.level2.antenna_metrics_agg_query(\n",
    "#                                                         table_name='\\\n",
    "#                                                         table_antenna_metrics_hourly_df',\n",
    "#                                                         columns='hour'))\n",
    "\n",
    "# # ### antenna_interactions\n",
    "# antenna_interactions_df = gn.aggregate_chunks(feature_type='interactions', attributes=att,\n",
    "#                                               sparksession=spark, cache_table=False,\n",
    "#                                               query=gn.queries.level2.antenna_interactions_agg_query(\n",
    "#                                                         table_name='\\\n",
    "#                                                         table_antenna_metrics_interactions_df'))\n",
    "\n",
    "# # ### bandicoot_features\n",
    "# if att.bc_flag and not att.hdfs_flag:\n",
    "#     # load in all items\n",
    "#     antenna_bandicoot_df = gn.sdf_from_folder(att.antenna_features_path+'bc/',\n",
    "#                                               attributes=att,\n",
    "#                                               sparksession=spark,\n",
    "#                                               recursive=True)\n",
    "\n",
    "#     # get total sum of weights (should be equal to total number of users!)\n",
    "#     sum_weights = antenna_bandicoot_df.select('count').groupBy().sum()\\\n",
    "#         .collect()[0][0]\n",
    "\n",
    "#     # apply weighing (multiply with weight and divide by all weights)\n",
    "#     antenna_bandicoot_df = antenna_bandicoot_df\\\n",
    "#         .select('antenna_id', *[(col(col_name)*col('count')/sum_weights)\n",
    "#                                 .alias(col_name) for col_name in\n",
    "#                                 antenna_bandicoot_df.columns[1:]])\n",
    "\n",
    "#     antenna_bandicoot_df = antenna_bandicoot_df.groupBy('antenna_id').\\\n",
    "#         sum().drop('sum(antenna_id)', 'sum(count)')  # sum up per antenna_id\n",
    "\n",
    "#     # renaming to more readable column names\n",
    "#     clean_cols = [c.replace('sum(', '').replace(')', '')\n",
    "#                   for c in antenna_bandicoot_df.columns]\n",
    "#     antenna_bandicoot_df = antenna_bandicoot_df.toDF(*clean_cols)\n",
    "\n",
    "#     print('Aggregated chunks for bandicoot!')\n",
    "\n",
    "# # **Level 3**: Intermediate feature tables\n",
    "# #\n",
    "# # Datasets holding different features/variables with columns in brackets:\n",
    "# # + **alltime_features**: *(antenna_id, calls_ratio, sms_ratio, vol_ratio,\n",
    "# #  sms2calls_ratio)*\n",
    "# # + **active_users_features**: *(antenna_id, active_users)*\n",
    "# # + **interaction_features**: *(antenna_id, calls_dist_mean, sms_dist_mean,\n",
    "# #  calls_isolation, sms_isolation, calls_entropy, sms_entropy, dist2c)*\n",
    "# # + **variance_features**: *(antenna_id, calls_ratio_var, sms_ratio_var,\n",
    "# #  vol_ratio_var)*\n",
    "# # + **daily_features**: *(antenna_id, og_calls_week_ratio, og_sms_week_ratio,\n",
    "# #  og_vol_week_ratio, ic_calls_week_ratio, ic_sms_week_ratio,\n",
    "# #  ic_vol_week_ratio)*\n",
    "# # + **hourly_features**: *(antenna_id, og_calls_work_ratio, og_sms_work_ratio,\n",
    "# #  og_vol_work_ratio, ic_calls_work_ratio, ic_sms_work_ratio,\n",
    "# #  ic_vol_work_ratio, og_calls_peak_ratio, og_sms_peak_ratio,\n",
    "# #  og_vol_peak_ratio, ic_calls_peak_ratio, ic_sms_peak_ratio,\n",
    "# #  ic_vol_peak_ratio)*\n",
    "# # + **antenna_bandicoot_features**: alltime averaged bandicoot interactions on\n",
    "# #  antenna level *(antenna_id, ...)*\n",
    "# #\n",
    "# # A further explanation of the single features can be found\n",
    "# #  here[http://bandicoot.mit.edu/docs/reference/bandicoot.individual.html]\n",
    "# print('Starting with Level 3: Intermediate feature tables.')\n",
    "# # ### alltime_features\n",
    "# alltime_features_df = spark.sql(gn.queries.level3.alltime_features_query(\n",
    "#                                 table_name='table_antenna_metrics_week_df'))\n",
    "\n",
    "# # ### active_users_features\n",
    "# active_users_features_df = home_antennas.select('antenna_id', 'user_id')\\\n",
    "#     .distinct().groupBy('antenna_id').count()\\\n",
    "#     .selectExpr('antenna_id', 'count as active_users')\n",
    "\n",
    "# # ### interaction_features\n",
    "# interaction_features_df = spark.sql(gn.queries.level3.interaction_features_query(\n",
    "#                                 table_name='\\\n",
    "#                                 table_antenna_metrics_interactions_df',\n",
    "#                                 c_coord=att.c_coord,\n",
    "#                                 n_home_antennas=n_home_antennas))\n",
    "\n",
    "# # ### variance_features\n",
    "# variance_features_df = spark.sql(gn.queries.level3.variance_features_query(\n",
    "#                                 table_name='table_antenna_metrics_week_df'))\n",
    "\n",
    "# # ### daily_features\n",
    "# daily_features_df = spark.sql(gn.queries.level3.daily_features_query(\n",
    "#                              table_name='table_antenna_metrics_week_df'))\n",
    "\n",
    "# # ### hourly_features\n",
    "# hourly_features_df = spark.sql(gn.queries.level3.hourly_features_query(\n",
    "#                                 table_name='table_antenna_metrics_hourly_df',\n",
    "#                                 work_day=att.work_day,\n",
    "#                                 early_peak=att.early_peak,\n",
    "#                                 late_peak=att.late_peak))\n",
    "\n",
    "# # **Level 4**: Final feature table\n",
    "# print('Starting with Level 4: Final feature table.')\n",
    "\n",
    "# final_features_df = antennas_locations\\\n",
    "#     .join(alltime_features_df, 'antenna_id', 'left')\\\n",
    "#     .join(active_users_features_df, 'antenna_id', 'left')\\\n",
    "#     .join(interaction_features_df, 'antenna_id', 'left')\\\n",
    "#     .join(variance_features_df, 'antenna_id', 'left')\\\n",
    "#     .join(daily_features_df, 'antenna_id', 'left')\\\n",
    "#     .join(hourly_features_df, 'antenna_id', 'left')\n",
    "\n",
    "# if att.bc_flag and not att.hdfs_flag:\n",
    "#     final_features_df = final_features_df.join(antenna_bandicoot_df,\n",
    "#                                                'antenna_id', 'left')\n",
    "\n",
    "# # ## save final output\n",
    "# final_features_df.coalesce(1).write.csv('final_features', mode='overwrite',\n",
    "#                                         header=True)\n",
    "# # rename final file if local\n",
    "# if not att.hdfs_flag:\n",
    "#     os.rename(glob.glob('final_features/*.csv')[0], 'final_features.csv')\n",
    "#     shutil.rmtree('final_features')\n",
    "\n",
    "# spark.stop()\n",
    "# if att.clean_up:\n",
    "#     if att.hdfs_flag:\n",
    "#             (ret, out, err)= gn.run_cmd(['hdfs', 'dfs', '-rm', '-R', att.antenna_features_path])\n",
    "#     else:\n",
    "#         shutil.rmtree(att.antenna_features_path)\n",
    "\n",
    "# print('DONE! Features file saved to final_features/.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
